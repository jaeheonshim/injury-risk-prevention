{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Football Injury Datasets\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_injuries.html\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_rosters.html\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_players.html\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_combine.html\n",
    "\n",
    "https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n",
    "\n",
    "https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n",
    "\n",
    "https://nflreadr.nflverse.com/articles/dictionary_combine.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nfl_data_py as nfl\n",
    "\n",
    "# combined = nfl.import_combine_data([2009, 2024])\n",
    "# cleancombined = nfl.clean_nfl_data(combined)\n",
    "# print(\"COMBINE DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleancombined.columns.tolist())\n",
    "\n",
    "# rosters = nfl.import_seasonal_rosters([2009, 2024])\n",
    "# cleanrosters = nfl.clean_nfl_data(rosters)\n",
    "# print(\"ROSTER DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleanrosters.columns.tolist())\n",
    "\n",
    "# ids = nfl.import_ids()\n",
    "# cleanids = nfl.clean_nfl_data(ids)\n",
    "# print(\"ID DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleanids.columns.tolist())\n",
    "\n",
    "# injuries = nfl.import_injuries([2009, 2024])\n",
    "# cleaninjuries = nfl.clean_nfl_data(injuries)\n",
    "# print(\"INJURY DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleaninjuries.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nfl_data_py as nfl\n",
    "\n",
    "# # ---------------------------\n",
    "# # 1. Import Data and Standardize Keys\n",
    "# # ---------------------------\n",
    "# years = list(range(2009, 2025))\n",
    "\n",
    "# # Import and clean data\n",
    "# combine_df = nfl.clean_nfl_data(nfl.import_combine_data(years=years))\n",
    "# rosters_df = nfl.clean_nfl_data(nfl.import_seasonal_rosters(years=years))\n",
    "# injuries_df = nfl.clean_nfl_data(nfl.import_injuries(years=years))\n",
    "\n",
    "# # Create a common key (using player_name for combine/roster and full_name for injuries)\n",
    "# combine_df['player_key'] = combine_df['player_name'].str.lower().str.strip()\n",
    "# rosters_df['player_key'] = rosters_df['player_name'].str.lower().str.strip()\n",
    "# injuries_df['player_key'] = injuries_df['full_name'].str.lower().str.strip()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2. Prepare Injuries Data\n",
    "# # ---------------------------\n",
    "# # Rename injuries columns to avoid conflicts\n",
    "# injuries_df = injuries_df.rename(columns={\n",
    "#     'season': 'season_injury', \n",
    "#     'week': 'week_injury',\n",
    "#     'position': 'position_injury'\n",
    "# })\n",
    "\n",
    "# # Define valid injury types (all lowercase)\n",
    "# valid_injury_types = [\n",
    "#     \"knee\", \"ankle\", \"hamstring\", \"shoulder\", \"foot\", \"concussion\",\n",
    "#     \"groin\", \"back\", \"calf\", \"hip\", \"neck\", \"toe\", \"quadricep\", \"elbow\",\n",
    "#     \"hand\", \"rib\", \"wrist\", \"thumb\", \"abdomen\", \"head\", \"finger\", \"achilles\",\n",
    "#     \"shin\", \"pectoral\", \"forearm\", \"heel\", \"biceps\", \"fibula\"\n",
    "# ]\n",
    "\n",
    "# # Create one-hot indicator columns based on practice injury descriptions.\n",
    "# # (Using both 'practice_primary_injury' and 'practice_secondary_injury')\n",
    "# for inj in valid_injury_types:\n",
    "#     col_name = f\"injury_{inj}\"\n",
    "#     injuries_df[col_name] = (\n",
    "#         injuries_df['practice_primary_injury'].str.lower().str.contains(inj, na=False) |\n",
    "#         injuries_df['practice_secondary_injury'].str.lower().str.contains(inj, na=False)\n",
    "#     ).astype(int)\n",
    "\n",
    "# # List of injury indicator column names (these will be our target columns)\n",
    "# injury_type_cols = [f\"injury_{inj}\" for inj in valid_injury_types]\n",
    "\n",
    "# # Filter out rows that don't mention any valid injury type\n",
    "# injuries_df = injuries_df[injuries_df[injury_type_cols].sum(axis=1) > 0]\n",
    "\n",
    "# # Sort injuries by season_injury and week_injury (earlier events first)\n",
    "# injuries_df = injuries_df.sort_values(by=['season_injury', 'week_injury'])\n",
    "\n",
    "# # Compute cumulative injury counts for each type for each player\n",
    "# for inj in valid_injury_types:\n",
    "#     cum_col = f\"cum_injury_{inj}\"\n",
    "#     injuries_df[cum_col] = injuries_df.groupby('player_key')[f\"injury_{inj}\"].cumsum()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3. Merge Datasets\n",
    "# # ---------------------------\n",
    "# # Merge roster and injury data on player_key and matching season (roster 'season' with injuries 'season_injury')\n",
    "# roster_injury_df = pd.merge(\n",
    "#     rosters_df,\n",
    "#     injuries_df,\n",
    "#     left_on=['player_key', 'season'],\n",
    "#     right_on=['player_key', 'season_injury'],\n",
    "#     how='outer',\n",
    "#     suffixes=('_roster', '_injury')\n",
    "# )\n",
    "\n",
    "# # Merge in combine data on player_key (drop combine's 'season' to avoid conflict)\n",
    "# merged_df = pd.merge(\n",
    "#     roster_injury_df,\n",
    "#     combine_df.drop(columns=['season']),\n",
    "#     on='player_key',\n",
    "#     how='outer',\n",
    "#     suffixes=('', '_combine')\n",
    "# )\n",
    "\n",
    "# # ---------------------------\n",
    "# # 4. Cleanup and One-Hot Encode Position\n",
    "# # ---------------------------\n",
    "# # Drop columns we no longer need\n",
    "# cols_to_drop = ['report_status', 'practice_primary_injury', 'practice_secondary_injury', 'prev_injury_count', 'position_injury']\n",
    "# required_columns = ['height', 'weight', 'years_exp', 'age', 'practice_primary_injury']\n",
    "# merged_df = merged_df.dropna(subset=required_columns)\n",
    "# merged_df = merged_df.drop(columns=[col for col in cols_to_drop if col in merged_df.columns])\n",
    "\n",
    "# # One-hot encode the player's position (from roster data; assumed column 'position' exists)\n",
    "# position_dummies = pd.get_dummies(merged_df['position'], prefix='pos')\n",
    "# merged_df = pd.concat([merged_df, position_dummies], axis=1)\n",
    "\n",
    "# # ---------------------------\n",
    "# # 5. Subset Final Columns with Injury Indicators at the End\n",
    "# # ---------------------------\n",
    "# # Define features (all columns except the injury indicator ones)\n",
    "# # Here we choose: 'height', 'weight', 'years_exp', 'age', cumulative injury counts, and position dummies.\n",
    "# feature_columns = ['height', 'weight', 'years_exp', 'age'] + \\\n",
    "#                   [f\"cum_injury_{inj}\" for inj in valid_injury_types] + \\\n",
    "#                   list(position_dummies.columns)\n",
    "# # Target columns (the injury indicator one-hot columns)\n",
    "# target_columns = injury_type_cols\n",
    "\n",
    "# # Set the desired column order: features first, then targets\n",
    "# desired_columns = feature_columns + target_columns\n",
    "\n",
    "# final_subset = merged_df[desired_columns]\n",
    "\n",
    "# # ---------------------------\n",
    "# # 6. Export and Display\n",
    "# # ---------------------------\n",
    "# final_subset.to_csv(\"train.csv\", index=False)\n",
    "# print(\"Data exported to train.csv\")\n",
    "# print(\"Final subset column names:\")\n",
    "# print(final_subset.columns.tolist())\n",
    "\n",
    "# # ---------------------------\n",
    "# # Splitting into Features and Targets\n",
    "# # ---------------------------\n",
    "# # To split into features (X) and targets (y), you could do:\n",
    "# X = final_subset[feature_columns]\n",
    "# y = final_subset[target_columns]\n",
    "\n",
    "# print(\"\\nFeatures (first few rows):\")\n",
    "# print(X.head())\n",
    "# print(\"\\nTargets (first few rows):\")\n",
    "# print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load the dataset that was previously exported to train.csv\n",
    "# data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# # Randomly split the data into 80% training and 20% testing sets.\n",
    "# # Setting random_state ensures reproducibility.\n",
    "# train_data, test_data = train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "# # Export the splits to separate CSV files\n",
    "# train_data.to_csv(\"train_data.csv\", index=False)\n",
    "# test_data.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "# print(\"Training and testing data have been saved as 'train_data.csv' and 'test_data.csv', respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import joblib\n",
    "\n",
    "# # Load the pre‚Äêsplit datasets (we assume you have train_data.csv and test_data.csv)\n",
    "# train_data = pd.read_csv(\"train_data.csv\")\n",
    "# test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# # The first 59 columns are features, the remaining are targets.\n",
    "# X_train = train_data.iloc[:, :59]\n",
    "# y_train = train_data.iloc[:, 59:]\n",
    "# X_test  = test_data.iloc[:, :59]\n",
    "# y_test  = test_data.iloc[:, 59:]\n",
    "\n",
    "# # Define a parameter grid for hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Create a RandomForestRegressor model\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Set up GridSearchCV for multioutput regression\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation R^2 score:\", grid_search.best_score_)\n",
    "\n",
    "# # Use the best estimator to predict and evaluate on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# test_score = best_model.score(X_test, y_test)\n",
    "# print(\"Test R^2 score:\", test_score)\n",
    "\n",
    "# # Persist (export) the best model\n",
    "# joblib.dump(best_model, \"injury_model_tuned.joblib\")\n",
    "# print(\"Tuned model saved to 'injury_model_tuned.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# # Step 1: Load your saved model\n",
    "# model = joblib.load('RandomForestRegressor.joblib')\n",
    "\n",
    "# # Step 2: Prepare a custom input sample\n",
    "# # Replace the values below with your actual test values.\n",
    "# # Make sure the input array has the same number and order of features as used during training.\n",
    "# custom_input = np.array([[76, 290, 5, 28,\n",
    "#     0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, \n",
    "#     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "\n",
    "# # Step 3: Use the model to predict the output\n",
    "# predicted_output = model.predict(custom_input)\n",
    "\n",
    "# print(\"Predicted injury outputs:\", predicted_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hackalytics25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
