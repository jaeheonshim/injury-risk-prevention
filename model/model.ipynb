{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Football Injury Datasets Source\n",
    "\n",
    "https://github.com/nflverse/nfl_data_py\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_injuries.html\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_rosters.html\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_players.html\n",
    "\n",
    "https://nflreadr.nflverse.com/reference/load_combine.html\n",
    "\n",
    "https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n",
    "\n",
    "https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n",
    "\n",
    "https://nflreadr.nflverse.com/articles/dictionary_combine.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nfl_data_py as nfl\n",
    "\n",
    "# combined = nfl.import_combine_data([2009, 2024])\n",
    "# cleancombined = nfl.clean_nfl_data(combined)\n",
    "# print(\"COMBINE DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleancombined.columns.tolist())\n",
    "\n",
    "# rosters = nfl.import_seasonal_rosters([2009, 2024])\n",
    "# cleanrosters = nfl.clean_nfl_data(rosters)\n",
    "# print(\"ROSTER DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleanrosters.columns.tolist())\n",
    "\n",
    "# ids = nfl.import_ids()\n",
    "# cleanids = nfl.clean_nfl_data(ids)\n",
    "# print(\"ID DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleanids.columns.tolist())\n",
    "\n",
    "# injuries = nfl.import_injuries([2009, 2024])\n",
    "# cleaninjuries = nfl.clean_nfl_data(injuries)\n",
    "# print(\"INJURY DATA\")\n",
    "# print(\"====================================\")\n",
    "# print(cleaninjuries.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nfl_data_py as nfl\n",
    "\n",
    "# # ---------------------------\n",
    "# # 1. Import Data and Standardize Keys\n",
    "# # ---------------------------\n",
    "# years = list(range(2009, 2025))\n",
    "\n",
    "# # Import and clean data\n",
    "# combine_df = nfl.clean_nfl_data(nfl.import_combine_data(years=years))\n",
    "# rosters_df = nfl.clean_nfl_data(nfl.import_seasonal_rosters(years=years))\n",
    "# injuries_df = nfl.clean_nfl_data(nfl.import_injuries(years=years))\n",
    "\n",
    "# # Create a common key (using player_name for combine/roster and full_name for injuries)\n",
    "# combine_df['player_key'] = combine_df['player_name'].str.lower().str.strip()\n",
    "# rosters_df['player_key'] = rosters_df['player_name'].str.lower().str.strip()\n",
    "# injuries_df['player_key'] = injuries_df['full_name'].str.lower().str.strip()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2. Prepare Injuries Data\n",
    "# # ---------------------------\n",
    "# # Rename injuries columns to avoid conflicts\n",
    "# injuries_df = injuries_df.rename(columns={\n",
    "#     'season': 'season_injury', \n",
    "#     'week': 'week_injury',\n",
    "#     'position': 'position_injury'\n",
    "# })\n",
    "\n",
    "# # Define valid injury types (all lowercase)\n",
    "# valid_injury_types = [\n",
    "#     \"knee\", \"ankle\", \"hamstring\", \"shoulder\", \"foot\", \"concussion\",\n",
    "#     \"groin\", \"back\", \"calf\", \"hip\", \"neck\", \"toe\", \"quadricep\", \"elbow\",\n",
    "#     \"hand\", \"rib\", \"wrist\", \"thumb\", \"abdomen\", \"head\", \"finger\", \"achilles\",\n",
    "#     \"shin\", \"pectoral\", \"forearm\", \"heel\", \"biceps\", \"fibula\"\n",
    "# ]\n",
    "\n",
    "# # Create one-hot indicator columns based on practice injury descriptions.\n",
    "# # (Using both 'practice_primary_injury' and 'practice_secondary_injury')\n",
    "# for inj in valid_injury_types:\n",
    "#     col_name = f\"injury_{inj}\"\n",
    "#     injuries_df[col_name] = (\n",
    "#         injuries_df['practice_primary_injury'].str.lower().str.contains(inj, na=False) |\n",
    "#         injuries_df['practice_secondary_injury'].str.lower().str.contains(inj, na=False)\n",
    "#     ).astype(int)\n",
    "\n",
    "# # List of injury indicator column names (these will be our target columns)\n",
    "# injury_type_cols = [f\"injury_{inj}\" for inj in valid_injury_types]\n",
    "\n",
    "# # Filter out rows that don't mention any valid injury type\n",
    "# injuries_df = injuries_df[injuries_df[injury_type_cols].sum(axis=1) > 0]\n",
    "\n",
    "# # Sort injuries by season_injury and week_injury (earlier events first)\n",
    "# injuries_df = injuries_df.sort_values(by=['season_injury', 'week_injury'])\n",
    "\n",
    "# # Compute cumulative injury counts for each type for each player\n",
    "# for inj in valid_injury_types:\n",
    "#     cum_col = f\"cum_injury_{inj}\"\n",
    "#     injuries_df[cum_col] = injuries_df.groupby('player_key')[f\"injury_{inj}\"].cumsum()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3. Merge Datasets\n",
    "# # ---------------------------\n",
    "# # Merge roster and injury data on player_key and matching season (roster 'season' with injuries 'season_injury')\n",
    "# roster_injury_df = pd.merge(\n",
    "#     rosters_df,\n",
    "#     injuries_df,\n",
    "#     left_on=['player_key', 'season'],\n",
    "#     right_on=['player_key', 'season_injury'],\n",
    "#     how='outer',\n",
    "#     suffixes=('_roster', '_injury')\n",
    "# )\n",
    "\n",
    "# # Merge in combine data on player_key (drop combine's 'season' to avoid conflict)\n",
    "# merged_df = pd.merge(\n",
    "#     roster_injury_df,\n",
    "#     combine_df.drop(columns=['season']),\n",
    "#     on='player_key',\n",
    "#     how='outer',\n",
    "#     suffixes=('', '_combine')\n",
    "# )\n",
    "\n",
    "# # ---------------------------\n",
    "# # 4. Cleanup and One-Hot Encode Position\n",
    "# # ---------------------------\n",
    "# # Drop columns we no longer need\n",
    "# cols_to_drop = ['report_status', 'practice_primary_injury', 'practice_secondary_injury', 'prev_injury_count', 'position_injury']\n",
    "# required_columns = ['height', 'weight', 'years_exp', 'age', 'practice_primary_injury']\n",
    "# merged_df = merged_df.dropna(subset=required_columns)\n",
    "# merged_df = merged_df.drop(columns=[col for col in cols_to_drop if col in merged_df.columns])\n",
    "\n",
    "# # One-hot encode the player's position (from roster data; assumed column 'position' exists)\n",
    "# position_dummies = pd.get_dummies(merged_df['position'], prefix='pos')\n",
    "# merged_df = pd.concat([merged_df, position_dummies], axis=1)\n",
    "\n",
    "# # ---------------------------\n",
    "# # 5. Subset Final Columns with Injury Indicators at the End\n",
    "# # ---------------------------\n",
    "# # Define features (all columns except the injury indicator ones)\n",
    "# # Here we choose: 'height', 'weight', 'years_exp', 'age', cumulative injury counts, and position dummies.\n",
    "# feature_columns = ['height', 'weight', 'years_exp', 'age'] + \\\n",
    "#                   [f\"cum_injury_{inj}\" for inj in valid_injury_types] + \\\n",
    "#                   list(position_dummies.columns)\n",
    "# # Target columns (the injury indicator one-hot columns)\n",
    "# target_columns = injury_type_cols\n",
    "\n",
    "# # Set the desired column order: features first, then targets\n",
    "# desired_columns = feature_columns + target_columns\n",
    "\n",
    "# final_subset = merged_df[desired_columns]\n",
    "\n",
    "# # ---------------------------\n",
    "# # 6. Export and Display\n",
    "# # ---------------------------\n",
    "# final_subset.to_csv(\"train.csv\", index=False)\n",
    "# print(\"Data exported to train.csv\")\n",
    "# print(\"Final subset column names:\")\n",
    "# print(final_subset.columns.tolist())\n",
    "\n",
    "# # ---------------------------\n",
    "# # Splitting into Features and Targets\n",
    "# # ---------------------------\n",
    "# # To split into features (X) and targets (y), you could do:\n",
    "# X = final_subset[feature_columns]\n",
    "# y = final_subset[target_columns]\n",
    "\n",
    "# print(\"\\nFeatures (first few rows):\")\n",
    "# print(X.head())\n",
    "# print(\"\\nTargets (first few rows):\")\n",
    "# print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load the dataset that was previously exported to train.csv\n",
    "# data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# # Randomly split the data into 80% training and 20% testing sets.\n",
    "# # Setting random_state ensures reproducibility.\n",
    "# train_data, test_data = train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "# # Export the splits to separate CSV files\n",
    "# train_data.to_csv(\"train_data.csv\", index=False)\n",
    "# test_data.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "# print(\"Training and testing data have been saved as 'train_data.csv' and 'test_data.csv', respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 5.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 5.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 5.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 9.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 9.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 9.9min\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Best cross-validation R^2 score: 0.5379267872628422\n",
      "Test R^2 score: 0.579156882855993\n",
      "Tuned model saved to 'RandomForestRegressor3.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import joblib\n",
    "\n",
    "# # Load the pre‐split datasets\n",
    "# train_data = pd.read_csv(\"train_data.csv\")\n",
    "# test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# # The first 59 columns are features, the remaining are targets.\n",
    "# X_train = train_data.iloc[:, :59]\n",
    "# y_train = train_data.iloc[:, 59:]\n",
    "# X_test  = test_data.iloc[:, :59]\n",
    "# y_test  = test_data.iloc[:, 59:]\n",
    "\n",
    "# # Define a parameter grid for hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'n_estimators': [500, 1000],\n",
    "#     'max_depth': [None],\n",
    "#     'min_samples_split': [2],\n",
    "#     'min_samples_leaf': [1]\n",
    "# }\n",
    "\n",
    "# # Create a RandomForestRegressor model\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Set up GridSearchCV for multioutput regression\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation R^2 score:\", grid_search.best_score_)\n",
    "\n",
    "# # Use the best estimator to predict and evaluate on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# test_score = best_model.score(X_test, y_test)\n",
    "# print(\"Test R^2 score:\", test_score)\n",
    "\n",
    "# # Persist (export) the best model\n",
    "# joblib.dump(best_model, \"RandomForestRegressor3.joblib\")\n",
    "# print(\"Tuned model saved to 'RandomForestRegressor3.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import r2_score\n",
    "# import joblib\n",
    "\n",
    "# # Load the pre‐split datasets\n",
    "# train_data = pd.read_csv(\"train_data.csv\")\n",
    "# test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# # The first 59 columns are features, the remaining are targets.\n",
    "# X_train = train_data.iloc[:, :59]\n",
    "# y_train = train_data.iloc[:, 59:]\n",
    "# X_test  = test_data.iloc[:, :59]\n",
    "# y_test  = test_data.iloc[:, 59:]\n",
    "\n",
    "# # Define a parameter grid for KNeighborsRegressor\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'p': [1, 2]  # p=1: Manhattan distance, p=2: Euclidean distance\n",
    "# }\n",
    "\n",
    "# # Create the KNeighborsRegressor instance\n",
    "# knn = KNeighborsRegressor()\n",
    "\n",
    "# print(\"Starting GridSearchCV\")\n",
    "# # Set up GridSearchCV with 5-fold cross-validation and R² as the scoring metric.\n",
    "# grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# # Run the grid search on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Output the best parameters and the best cross-validation R² score\n",
    "# print(\"Best parameters found:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation R² score:\", grid_search.best_score_)\n",
    "\n",
    "# # Retrieve the best estimator and evaluate on the test set\n",
    "# best_knn = grid_search.best_estimator_\n",
    "# test_r2 = best_knn.score(X_test, y_test)\n",
    "# print(\"Test set R² score:\", test_r2)\n",
    "\n",
    "# # Persist (export) the best model\n",
    "# joblib.dump(best_knn, \"KNearestNeighbours.joblib\")\n",
    "# print(\"Tuned model saved to 'KNearestNeighbours.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "# from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import r2_score\n",
    "# import joblib\n",
    "\n",
    "# # Load the pre-split data\n",
    "# train_data = pd.read_csv(\"train_data.csv\")\n",
    "# test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# # Assume the first 59 columns are features, remaining are continuous injury risks\n",
    "# X_train = train_data.iloc[:, :59].values\n",
    "# y_train = train_data.iloc[:, 59:].values\n",
    "# X_test  = test_data.iloc[:, :59].values\n",
    "# y_test  = test_data.iloc[:, 59:].values\n",
    "\n",
    "# # Initialize the base HistGradientBoostingRegressor\n",
    "# hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# # Wrap it in MultiOutputRegressor to handle multioutput regression\n",
    "# model = MultiOutputRegressor(hgb)\n",
    "\n",
    "# # Define a parameter grid; note the prefix for the underlying regressor\n",
    "# param_grid = {\n",
    "#     'estimator__max_iter': [100, 200],\n",
    "#     'estimator__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'estimator__max_depth': [None, 5, 10]\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=model, \n",
    "#                            param_grid=param_grid, \n",
    "#                            cv=3, \n",
    "#                            scoring='r2', \n",
    "#                            n_jobs=2, \n",
    "#                            verbose=2)\n",
    "\n",
    "# # Fit the grid search on the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best cross-validation R^2 score:\", grid_search.best_score_)\n",
    "\n",
    "# # Evaluate the best estimator on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# test_score = best_model.score(X_test, y_test)\n",
    "# print(\"Test R^2 score:\", test_score)\n",
    "\n",
    "# # Persist (export) the best model using joblib\n",
    "# joblib.dump(best_model, \"HistGradientBoostingRegressor.joblib\")\n",
    "# print(\"Tuned model saved to 'HistGradientBoostingRegressor.joblib'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process\n",
      "Test R^2 score: 0.6022060455868832\n",
      "Model saved to 'ExtraTreesRegressor4.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.metrics import r2_score\n",
    "# import joblib\n",
    "\n",
    "# # Load the pre-split datasets\n",
    "# train_data = pd.read_csv(\"train_data.csv\")\n",
    "# test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# # Assume the first 59 columns are features, and the remaining columns are the continuous injury targets.\n",
    "# X_train = train_data.iloc[:, :59].values\n",
    "# y_train = train_data.iloc[:, 59:].values\n",
    "# X_test  = test_data.iloc[:, :59].values\n",
    "# y_test  = test_data.iloc[:, 59:].values\n",
    "\n",
    "# # Initialize the ExtraTreesRegressor with the given parameters:\n",
    "# etr = ExtraTreesRegressor(n_estimators=50, \n",
    "#                            max_depth=None, \n",
    "#                            min_samples_split=2, \n",
    "#                            min_samples_leaf=1, \n",
    "#                            random_state=69)\n",
    "\n",
    "# print(\"Starting process\")\n",
    "# # Fit the model on training data\n",
    "# etr.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# y_pred = etr.predict(X_test)\n",
    "# test_score = r2_score(y_test, y_pred)\n",
    "# print(\"Test R^2 score:\", test_score)\n",
    "\n",
    "# # Persist the model using joblib\n",
    "# joblib.dump(etr, \"ExtraTreesRegressor4.joblib\")\n",
    "# print(\"Model saved to 'ExtraTreesRegressor4.joblib'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor R² score: 0.5780\n",
      "RandomForestRegressor2 R² score: 0.5547\n",
      "KNeighborsRegressor R² score: 0.4344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heuryn/anaconda3/envs/Hackalytics25/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor R² score: 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heuryn/anaconda3/envs/Hackalytics25/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor3 R² score: 0.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heuryn/anaconda3/envs/Hackalytics25/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor4 R² score: 0.6022\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "X_test  = test_data.iloc[:, :59]\n",
    "y_test  = test_data.iloc[:, 59:]\n",
    "\n",
    "rf_model = joblib.load('RandomForestRegressor.joblib')\n",
    "rf2_model = joblib.load('RandomForestRegressor2.joblib')\n",
    "knn_model = joblib.load('KNearestNeighbours.joblib')\n",
    "etr_model = joblib.load('ExtraTreesRegressor.joblib')\n",
    "etr3_model = joblib.load('ExtraTreesRegressor3.joblib')\n",
    "etr4_model = joblib.load('ExtraTreesRegressor4.joblib')\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"RandomForestRegressor\": rf_model,\n",
    "    \"RandomForestRegressor2\": rf2_model,\n",
    "    \"KNeighborsRegressor\": knn_model,\n",
    "    \"ExtraTreesRegressor\": etr_model,\n",
    "    \"ExtraTreesRegressor3\": etr3_model,\n",
    "    \"ExtraTreesRegressor4\": etr4_model\n",
    "}\n",
    "\n",
    "# Evaluate each model's R² score on the test set\n",
    "for name, model in models.items():\n",
    "    predictions = model.predict(X_test)\n",
    "    score = r2_score(y_test, predictions)\n",
    "    print(f\"{name} R² score: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hackalytics25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
